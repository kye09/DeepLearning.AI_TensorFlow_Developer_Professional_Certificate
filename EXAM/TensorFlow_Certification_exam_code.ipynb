{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1번"
      ],
      "metadata": {
        "id": "Luvsqselj8-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYoLLPRI-e-4",
        "outputId": "69510785-01e5-4bca-8ce3-be23ec0c8f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "    ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(1, input_shape=[1])\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='sgd', loss='mse')\n",
        "\n",
        "    model.fit(xs, ys, epochs = 1200, verbose = 0)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    return model\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2번"
      ],
      "metadata": {
        "id": "L-Dqg98okpdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def solution_model():\n",
        "    # 데이터 로드 및 shape 확인\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "    (x_train, y_train), (x_valid, y_valid) = fashion_mnist.load_data()\n",
        "\n",
        "    # 이미지 정규화\n",
        "    x_train = x_train/255.0\n",
        "    x_valid = x_valid/255.0\n",
        "\n",
        "    # modeling\n",
        "    model = Sequential([\n",
        "    Flatten(input_shape=(28,28)),\n",
        "    Dense(1024, activation = 'relu'),\n",
        "    Dense(516, activation = 'relu'),\n",
        "    Dense(256, activation = 'relu'),\n",
        "    Dense(128, activation = 'relu'),\n",
        "    Dense(64, activation = 'relu'),\n",
        "\n",
        "    Dense(10, activation = 'sigmoid')\n",
        "\n",
        "    ])\n",
        "\n",
        "    # 모델 컴파일 및 checkpoint callback 생성\n",
        "    model.compile(optimizer = 'adam', loss= 'sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "    checkpoint_path = \"my_checkpoint.ckpt\"\n",
        "    checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                save_weights_only=True,\n",
        "                                save_best_only=True,\n",
        "                                monitor='val_loss',\n",
        "                                verbose=1)\n",
        "\n",
        "    # 모델 fit\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        validation_data=(x_valid, y_valid),\n",
        "                        epochs=20,\n",
        "                        callbacks=[checkpoint],\n",
        "                      )\n",
        "    # 모델 가중치 저장\n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP-Vlj4TA3WN",
        "outputId": "b0587295-bd0e-4e79-b633-35c69503b42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/20\n",
            "1864/1875 [============================>.] - ETA: 0s - loss: 0.5050 - acc: 0.8185\n",
            "Epoch 1: val_loss improved from inf to 0.41561, saving model to my_checkpoint.ckpt\n",
            "1875/1875 [==============================] - 10s 4ms/step - loss: 0.5044 - acc: 0.8188 - val_loss: 0.4156 - val_acc: 0.8498\n",
            "Epoch 2/20\n",
            "1865/1875 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8633\n",
            "Epoch 2: val_loss did not improve from 0.41561\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3781 - acc: 0.8633 - val_loss: 0.4345 - val_acc: 0.8430\n",
            "Epoch 3/20\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8755\n",
            "Epoch 3: val_loss improved from 0.41561 to 0.37924, saving model to my_checkpoint.ckpt\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3393 - acc: 0.8754 - val_loss: 0.3792 - val_acc: 0.8654\n",
            "Epoch 4/20\n",
            "1869/1875 [============================>.] - ETA: 0s - loss: 0.3155 - acc: 0.8855\n",
            "Epoch 4: val_loss did not improve from 0.37924\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3154 - acc: 0.8855 - val_loss: 0.4468 - val_acc: 0.8393\n",
            "Epoch 5/20\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.8904\n",
            "Epoch 5: val_loss improved from 0.37924 to 0.36175, saving model to my_checkpoint.ckpt\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2964 - acc: 0.8904 - val_loss: 0.3618 - val_acc: 0.8717\n",
            "Epoch 6/20\n",
            "1862/1875 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.8968\n",
            "Epoch 6: val_loss improved from 0.36175 to 0.34822, saving model to my_checkpoint.ckpt\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2819 - acc: 0.8968 - val_loss: 0.3482 - val_acc: 0.8761\n",
            "Epoch 7/20\n",
            "1869/1875 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.8991\n",
            "Epoch 7: val_loss improved from 0.34822 to 0.34284, saving model to my_checkpoint.ckpt\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2716 - acc: 0.8989 - val_loss: 0.3428 - val_acc: 0.8768\n",
            "Epoch 8/20\n",
            "1863/1875 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9044\n",
            "Epoch 8: val_loss did not improve from 0.34284\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2593 - acc: 0.9044 - val_loss: 0.3482 - val_acc: 0.8806\n",
            "Epoch 9/20\n",
            "1864/1875 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9069\n",
            "Epoch 9: val_loss did not improve from 0.34284\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2482 - acc: 0.9068 - val_loss: 0.3626 - val_acc: 0.8773\n",
            "Epoch 10/20\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 0.2437 - acc: 0.9107\n",
            "Epoch 10: val_loss did not improve from 0.34284\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2436 - acc: 0.9107 - val_loss: 0.3644 - val_acc: 0.8843\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.2294 - acc: 0.9135\n",
            "Epoch 11: val_loss improved from 0.34284 to 0.34097, saving model to my_checkpoint.ckpt\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2294 - acc: 0.9135 - val_loss: 0.3410 - val_acc: 0.8891\n",
            "Epoch 12/20\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9168\n",
            "Epoch 12: val_loss improved from 0.34097 to 0.33311, saving model to my_checkpoint.ckpt\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2236 - acc: 0.9168 - val_loss: 0.3331 - val_acc: 0.8876\n",
            "Epoch 13/20\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9192\n",
            "Epoch 13: val_loss did not improve from 0.33311\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2134 - acc: 0.9192 - val_loss: 0.3842 - val_acc: 0.8839\n",
            "Epoch 14/20\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 0.9210\n",
            "Epoch 14: val_loss did not improve from 0.33311\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2117 - acc: 0.9210 - val_loss: 0.3768 - val_acc: 0.8921\n",
            "Epoch 15/20\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9235\n",
            "Epoch 15: val_loss did not improve from 0.33311\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2047 - acc: 0.9235 - val_loss: 0.3672 - val_acc: 0.8863\n",
            "Epoch 16/20\n",
            "1865/1875 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9249\n",
            "Epoch 16: val_loss did not improve from 0.33311\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1998 - acc: 0.9250 - val_loss: 0.3471 - val_acc: 0.8957\n",
            "Epoch 17/20\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9268\n",
            "Epoch 17: val_loss did not improve from 0.33311\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1907 - acc: 0.9268 - val_loss: 0.4048 - val_acc: 0.8883\n",
            "Epoch 18/20\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9264\n",
            "Epoch 18: val_loss did not improve from 0.33311\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1963 - acc: 0.9265 - val_loss: 0.4013 - val_acc: 0.8884\n",
            "Epoch 19/20\n",
            "1869/1875 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9315\n",
            "Epoch 19: val_loss did not improve from 0.33311\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1814 - acc: 0.9316 - val_loss: 0.3855 - val_acc: 0.8928\n",
            "Epoch 20/20\n",
            "1862/1875 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9306\n",
            "Epoch 20: val_loss did not improve from 0.33311\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1808 - acc: 0.9305 - val_loss: 0.3933 - val_acc: 0.8934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3번"
      ],
      "metadata": {
        "id": "mrlevCoHkzxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "#\n",
        "# BASIC DATASETS QUESTION\n",
        "#\n",
        "# Create a classifier for the German Traffic Signs dataset that classifies\n",
        "# images of traffic signs into 43 classes.\n",
        "# ==============================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# The dataset contains traffic sign boards from the streets captured into\n",
        "# image files. There are 43 unique classes in total. The images are of shape\n",
        "# (30,30,3).\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# We have already divided the data for training and validation.\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. preprocess()\n",
        "# 2. solution_model()\n",
        "#\n",
        "# Your code will fail to be graded if the following criteria are not met:\n",
        "# 1. The input shape of your model must be (30,30,3), because the testing\n",
        "#    infrastructure expects inputs according to this specification.\n",
        "# 2. The last layer of your model must be a Dense layer with 43 neurons\n",
        "#    activated by softmax since this dataset has 43 classes.\n",
        "#\n",
        "# HINT: Your neural network must have a validation accuracy of approximately\n",
        "# 0.95 or above on the normalized validation dataset for top marks.\n",
        "\n",
        "import urllib\n",
        "import zipfile\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "\n",
        "# This function downloads and extracts the dataset to the directory that\n",
        "# contains this file.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "# (unless you need to change https to http)\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/germantrafficsigns.zip'\n",
        "    urllib.request.urlretrieve(url, 'germantrafficsigns.zip')\n",
        "    with zipfile.ZipFile('germantrafficsigns.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def preprocess(image, label):\n",
        "    # NORMALIZE YOUR IMAGES HERE (HINT: Rescale by 1/.255)\n",
        "    image=image/255.0  # NORMALIZING IMAGES (RESCALING)\n",
        "    # label= label/255.0\n",
        "    return image, label\n",
        "\n",
        "\n",
        "# This function loads the data, normalizes and resizes the images, splits it into\n",
        "# train and validation sets, defines the model, compiles it and finally\n",
        "# trains the model. The trained model is returned from this function.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION.\n",
        "def solution_model():\n",
        "    # Downloads and extracts the dataset to the directory that\n",
        "    # contains this file.\n",
        "    download_and_extract_data()\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "    IMG_SIZE = 30\n",
        "\n",
        "    # The following code reads the training and validation data from their\n",
        "    # respective directories, resizes them into the specified image size\n",
        "    # and splits them into batches. You must fill in the image_size\n",
        "    # argument for both training and validation data.\n",
        "    # HINT: Image size is a tuple\n",
        "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory='train/',\n",
        "        label_mode='categorical',\n",
        "        image_size= (IMG_SIZE,IMG_SIZE) ,\n",
        "        batch_size = BATCH_SIZE)\n",
        "\n",
        "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory='validation/',\n",
        "        label_mode='categorical',\n",
        "        image_size= (IMG_SIZE,IMG_SIZE) ,\n",
        "        batch_size = BATCH_SIZE)\n",
        "\n",
        "    # Normalizes train and validation datasets using the\n",
        "    # preprocess() function.\n",
        "    # Also makes other calls, as evident from the code, to prepare them for\n",
        "    # training.\n",
        "    # Do not batch or resize the images in the dataset here since it's already\n",
        "    # been done previously.\n",
        "\n",
        "    train_ds = train_ds.map(\n",
        "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(\n",
        "        tf.data.experimental.AUTOTUNE)\n",
        "    val_ds = val_ds.map(\n",
        "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Code to define the model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # ADD LAYERS OF THE MODEL HERE\n",
        "        tf.keras.layers.Conv2D(filters=56,kernel_size=(3,3),activation='relu',input_shape=(30,30,3)),\n",
        "        tf.keras.layers.Conv2D(16, (3,3), activation = 'relu'),\n",
        "        Dropout(0.2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(43, activation='softmax')\n",
        "        # tf.keras.layers.Conv2D(8, (3,3), activation = 'relu', input_shape=(30,30,3)),\n",
        "        # tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        # tf.keras.layers.Conv2D(16, (3,3), activation = 'relu'),\n",
        "        # tf.keras.layers.MaxPooling2D(2,2),\n",
        "        # tf.keras.layers.Conv2D(32, (3,3), activation = 'relu'),\n",
        "        # tf.keras.layers.MaxPooling2D(2,2),\n",
        "        # tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),\n",
        "        # tf.keras.layers.MaxPooling2D(2,2),\n",
        "        # tf.keras.layers.Flatten(),\n",
        "        # tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "        # tf.keras.layers.Dense(43, activation='softmax')\n",
        "\n",
        "        # If you don't adhere to the instructions in the following comments,\n",
        "        # tests will fail to grade your model:\n",
        "        # The input layer of your model must have an input shape of\n",
        "        # (30,30,3).\n",
        "        # Make sure your last layer has 43 neurons activated by softmax.\n",
        "        # tf.keras.layers.Dense(43, activation=tf.nn.softmax)\n",
        "    ])\n",
        "\n",
        "    # Code to compile and train the model\n",
        "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    model.fit(train_ds, validation_data=val_ds, epochs=20, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "metadata": {
        "id": "-gtvGo-OkzDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4번"
      ],
      "metadata": {
        "id": "II5ghgCokCVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# Build and train a classifier for the sarcasm dataset.\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown.\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# and you will be scored on whether sarcasm was correctly detected in those sentences.\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs = {}):\n",
        "    if(logs.get('accuracy') > 0.95 and logs.get('val_accuracy') > 0.8):\n",
        "      print(\"\\nDesired accuracy is achieved.\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"\"\n",
        "    training_size = 20000\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    # YOUR CODE HERE\n",
        "    with open(\"./sarcasm.json\", 'r') as f:\n",
        "      datastore = json.load(f)\n",
        "\n",
        "    for item in datastore:\n",
        "      sentences.append(item['headline'])\n",
        "      labels.append(item['is_sarcastic'])\n",
        "\n",
        "    training_sentences = sentences[0:training_size]\n",
        "    testing_sentences = sentences[training_size:]\n",
        "    training_labels = labels[0:training_size]\n",
        "    testing_labels = labels[training_size:]\n",
        "\n",
        "    tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
        "    tokenizer.fit_on_texts(training_sentences)\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "    training_padded = pad_sequences(training_sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n",
        "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "    testing_padded = pad_sequences(testing_sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n",
        "\n",
        "    training_labels = np.array(training_labels)\n",
        "    testing_labels = np.array(testing_labels)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "    # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Conv1D(64, 5, activation = 'relu'),\n",
        "        tf.keras.layers.MaxPooling1D(pool_size = 4),\n",
        "        tf.keras.layers.LSTM(64),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    callback = myCallback()\n",
        "\n",
        "    model.compile(\n",
        "        loss = 'binary_crossentropy',\n",
        "        optimizer = 'adam',\n",
        "        metrics = ['accuracy'])\n",
        "\n",
        "    model.fit(\n",
        "        training_padded,\n",
        "        training_labels,\n",
        "        epochs = 100,\n",
        "        validation_data = (\n",
        "            testing_padded,\n",
        "            testing_labels),\n",
        "        callbacks = callback)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_1q_3tCCRwM",
        "outputId": "9715ddcd-3b6c-4cea-db14-06cbdc2aae68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "625/625 [==============================] - 24s 26ms/step - loss: 0.6601 - accuracy: 0.6007 - val_loss: 0.6907 - val_accuracy: 0.5637\n",
            "Epoch 2/100\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.6650 - accuracy: 0.5991 - val_loss: 0.5335 - val_accuracy: 0.7645\n",
            "Epoch 3/100\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.4659 - accuracy: 0.7900 - val_loss: 0.4224 - val_accuracy: 0.8082\n",
            "Epoch 4/100\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.3959 - accuracy: 0.8238 - val_loss: 0.4004 - val_accuracy: 0.8185\n",
            "Epoch 5/100\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.3588 - accuracy: 0.8427 - val_loss: 0.3858 - val_accuracy: 0.8237\n",
            "Epoch 6/100\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.3333 - accuracy: 0.8547 - val_loss: 0.3834 - val_accuracy: 0.8281\n",
            "Epoch 7/100\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.3165 - accuracy: 0.8626 - val_loss: 0.3918 - val_accuracy: 0.8164\n",
            "Epoch 8/100\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.3038 - accuracy: 0.8702 - val_loss: 0.3966 - val_accuracy: 0.8155\n",
            "Epoch 9/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2849 - accuracy: 0.8770 - val_loss: 0.4001 - val_accuracy: 0.8264\n",
            "Epoch 10/100\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2749 - accuracy: 0.8822 - val_loss: 0.3943 - val_accuracy: 0.8210\n",
            "Epoch 11/100\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2601 - accuracy: 0.8892 - val_loss: 0.4439 - val_accuracy: 0.8214\n",
            "Epoch 12/100\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2490 - accuracy: 0.8950 - val_loss: 0.4326 - val_accuracy: 0.8240\n",
            "Epoch 13/100\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2361 - accuracy: 0.9013 - val_loss: 0.4486 - val_accuracy: 0.8213\n",
            "Epoch 14/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2281 - accuracy: 0.9065 - val_loss: 0.4182 - val_accuracy: 0.8232\n",
            "Epoch 15/100\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2261 - accuracy: 0.9061 - val_loss: 0.4654 - val_accuracy: 0.8150\n",
            "Epoch 16/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2166 - accuracy: 0.9100 - val_loss: 0.4493 - val_accuracy: 0.8144\n",
            "Epoch 17/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2050 - accuracy: 0.9151 - val_loss: 0.4534 - val_accuracy: 0.8202\n",
            "Epoch 18/100\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2016 - accuracy: 0.9140 - val_loss: 0.4558 - val_accuracy: 0.8243\n",
            "Epoch 19/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1962 - accuracy: 0.9209 - val_loss: 0.4902 - val_accuracy: 0.8170\n",
            "Epoch 20/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1877 - accuracy: 0.9248 - val_loss: 0.4613 - val_accuracy: 0.8185\n",
            "Epoch 21/100\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1846 - accuracy: 0.9231 - val_loss: 0.5135 - val_accuracy: 0.8171\n",
            "Epoch 22/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1753 - accuracy: 0.9304 - val_loss: 0.5164 - val_accuracy: 0.8201\n",
            "Epoch 23/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1763 - accuracy: 0.9276 - val_loss: 0.5251 - val_accuracy: 0.8144\n",
            "Epoch 24/100\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1747 - accuracy: 0.9291 - val_loss: 0.4997 - val_accuracy: 0.8170\n",
            "Epoch 25/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1674 - accuracy: 0.9322 - val_loss: 0.5346 - val_accuracy: 0.8047\n",
            "Epoch 26/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2119 - accuracy: 0.9114 - val_loss: 0.4870 - val_accuracy: 0.8071\n",
            "Epoch 27/100\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2115 - accuracy: 0.9146 - val_loss: 0.4984 - val_accuracy: 0.8119\n",
            "Epoch 28/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1826 - accuracy: 0.9244 - val_loss: 0.5256 - val_accuracy: 0.8170\n",
            "Epoch 29/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1752 - accuracy: 0.9273 - val_loss: 0.5114 - val_accuracy: 0.8131\n",
            "Epoch 30/100\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.1616 - accuracy: 0.9334 - val_loss: 0.5324 - val_accuracy: 0.8143\n",
            "Epoch 31/100\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1609 - accuracy: 0.9344 - val_loss: 0.5100 - val_accuracy: 0.8143\n",
            "Epoch 32/100\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1635 - accuracy: 0.9324 - val_loss: 0.5181 - val_accuracy: 0.8126\n",
            "Epoch 33/100\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1506 - accuracy: 0.9393 - val_loss: 0.5242 - val_accuracy: 0.8086\n",
            "Epoch 34/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1489 - accuracy: 0.9412 - val_loss: 0.5358 - val_accuracy: 0.8044\n",
            "Epoch 35/100\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1539 - accuracy: 0.9378 - val_loss: 0.5484 - val_accuracy: 0.8141\n",
            "Epoch 36/100\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1528 - accuracy: 0.9382 - val_loss: 0.5563 - val_accuracy: 0.8156\n",
            "Epoch 37/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1465 - accuracy: 0.9402 - val_loss: 0.5533 - val_accuracy: 0.8067\n",
            "Epoch 38/100\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.1481 - accuracy: 0.9423 - val_loss: 0.5958 - val_accuracy: 0.8064\n",
            "Epoch 39/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1489 - accuracy: 0.9389 - val_loss: 0.5433 - val_accuracy: 0.8047\n",
            "Epoch 40/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1491 - accuracy: 0.9392 - val_loss: 0.5710 - val_accuracy: 0.8094\n",
            "Epoch 41/100\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1420 - accuracy: 0.9437 - val_loss: 0.5980 - val_accuracy: 0.8117\n",
            "Epoch 42/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1432 - accuracy: 0.9421 - val_loss: 0.5717 - val_accuracy: 0.8085\n",
            "Epoch 43/100\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1381 - accuracy: 0.9468 - val_loss: 0.5647 - val_accuracy: 0.8141\n",
            "Epoch 44/100\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1400 - accuracy: 0.9445 - val_loss: 0.6089 - val_accuracy: 0.8044\n",
            "Epoch 45/100\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.1390 - accuracy: 0.9445 - val_loss: 0.5879 - val_accuracy: 0.8079\n",
            "Epoch 46/100\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.1372 - accuracy: 0.9463 - val_loss: 0.5790 - val_accuracy: 0.8070\n",
            "Epoch 47/100\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1399 - accuracy: 0.9442 - val_loss: 0.6142 - val_accuracy: 0.8095\n",
            "Epoch 48/100\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1354 - accuracy: 0.9456 - val_loss: 0.5638 - val_accuracy: 0.8122\n",
            "Epoch 49/100\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1316 - accuracy: 0.9469 - val_loss: 0.5916 - val_accuracy: 0.8103\n",
            "Epoch 50/100\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1302 - accuracy: 0.9467 - val_loss: 0.5780 - val_accuracy: 0.8100\n",
            "Epoch 51/100\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1304 - accuracy: 0.9470 - val_loss: 0.6197 - val_accuracy: 0.8116\n",
            "Epoch 52/100\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1323 - accuracy: 0.9466 - val_loss: 0.6167 - val_accuracy: 0.8128\n",
            "Epoch 53/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.1245 - accuracy: 0.9506\n",
            "Desired accuracy is achieved.\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1245 - accuracy: 0.9506 - val_loss: 0.6119 - val_accuracy: 0.8076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5번"
      ],
      "metadata": {
        "id": "lPF2qRock5Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#release\n",
        "# ==============================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative to its\n",
        "# difficulty. So your Category 1 question will score significantly less than\n",
        "# your Category 5 question.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure. You do not need them to solve the question.\n",
        "#\n",
        "# WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "# recurrent_dropout argument (you can alternatively set it to 0),\n",
        "# since it has not been implemented in the cuDNN kernel and may\n",
        "# result in much longer training times.\n",
        "#\n",
        "# WARNING: Input and output shape requirements are laid down in the section\n",
        "# 'INSTRUCTIONS' below and also reiterated in code comments.\n",
        "# Please read them thoroughly. After submitting the trained model for scoring,\n",
        "# if you are receiving a score of 0 or an error, please recheck the input and\n",
        "# output shapes of the model to see if it exactly matches our requirements.\n",
        "# Grading infrastrcuture is very strict about the shape requirements. Most common\n",
        "# issues occur when the shapes are not matching our expectations.\n",
        "#\n",
        "# TIP: You can print the output of model.summary() to review the model\n",
        "# architecture, input and output shapes of each layer.\n",
        "# If you have made sure that you have matched the shape requirements\n",
        "# and all the other instructions we have laid down, and still\n",
        "# receive a bad score, you must work on improving your model.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "#\n",
        "# TIME SERIES QUESTION\n",
        "#\n",
        "# Build and train a neural network to predict the time indexed variable of\n",
        "# the univariate US diesel prices (On - Highway) All types for the period of\n",
        "# 1994 - 2021.\n",
        "# Using a window of past 10 observations of 1 feature , train the model\n",
        "# to predict the next 10 observations of that feature.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Original Source:\n",
        "# https://www.eia.gov/dnav/pet/pet_pri_gnd_dcus_nus_w.htm#\n",
        "#\n",
        "# For the purpose of the examination we have used the Diesel (On - Highway) -\n",
        "# All Types time series data for the period of 1994 - 2021 from the\n",
        "# aforementioned link. The dataset has 1 time indexed feature.\n",
        "# We have provided a cleaned version of the data.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. solution_model()\n",
        "#\n",
        "# You may receive a score of 0 or your code will fail to be graded if the\n",
        "# following criteria are not met:\n",
        "#\n",
        "# 1. Model input shape must be (BATCH_SIZE, N_PAST = 10, N_FEATURES = 1),\n",
        "#    since the testing infrastructure expects a window of past N_PAST = 10\n",
        "#    observations of the 1 feature to predict the next N_FUTURE = 10\n",
        "#    observations of the same feature.\n",
        "#\n",
        "# 2. Model output shape must be (BATCH_SIZE, N_FUTURE = 10, N_FEATURES = 1)\n",
        "#\n",
        "# 3. The last layer of your model must be a Dense layer with 1 neuron since\n",
        "#    the model is expected to predict observations of 1 feature.\n",
        "#\n",
        "# 4. Don't change the values of the following constants:\n",
        "#    SPLIT_TIME, N_FEATURES, BATCH_SIZE, N_PAST, N_FUTURE, SHIFT, in\n",
        "#    solution_model() (See code for additional note on BATCH_SIZE).\n",
        "#\n",
        "# 5. Code for normalizing the data is provided - don't change it.\n",
        "#    Changing the normalizing code will affect your score.\n",
        "#\n",
        "# 6. Code for converting the dataset into windows is provided - don't change it.\n",
        "#    Changing the windowing code will affect your score.\n",
        "#\n",
        "# 7. Code for setting the seed is provided - don't change it.\n",
        "#\n",
        "# Make sure that the model architecture and input, output shapes match our\n",
        "# requirements by printing model.summary() and reviewing its output.\n",
        "#\n",
        "# HINT: If you follow all the rules mentioned above and throughout this\n",
        "# question while training your neural network, there is a possibility that a\n",
        "# validation MAE of approximately 0.02 or less on the normalized validation\n",
        "# dataset may fetch you top marks.\n",
        "\n",
        "\n",
        "import urllib\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "\n",
        "# This function is used to map the time series dataset into windows of\n",
        "# features and respective targets, to prepare it for training and validation.\n",
        "# The first element of the first window will be the first element of\n",
        "# the dataset.\n",
        "#\n",
        "# Consecutive windows are constructed by shifting the starting position\n",
        "# of the first window forward, one at a time (indicated by shift=1).\n",
        "#\n",
        "# For a window of n_past number of observations of the time\n",
        "# indexed variable in the dataset, the target for the window is the next\n",
        "# n_future number of observations of the variable, after the\n",
        "# end of the window.\n",
        "\n",
        "# DO NOT CHANGE THIS.\n",
        "def windowed_dataset(series, batch_size, n_past=10, n_future=10, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "# This function loads the data from the CSV file, normalizes the data and\n",
        "# splits the dataset into train and validation data. It also uses\n",
        "# windowed_dataset() to split the data into windows of observations and\n",
        "# targets. Finally it defines, compiles and trains a neural network. This\n",
        "# function returns the final trained model.\n",
        "url = 'https://www.dropbox.com/s/eduk281didil1km/Weekly_U.S.Diesel_Retail_Prices.csv?dl=1'\n",
        "urllib.request.urlretrieve(url, 'Weekly_U.S.Diesel_Retail_Prices.csv')\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def solution_model():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Reads the dataset.\n",
        "    df = pd.read_csv('Weekly_U.S.Diesel_Retail_Prices.csv',infer_datetime_format=True, index_col='Week of', header=0)\n",
        "\n",
        "    # Number of features in the dataset. We use all features as predictors to\n",
        "    # predict all features of future time steps.\n",
        "    N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
        "\n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "\n",
        "    # Splits the data into training and validation sets.\n",
        "    SPLIT_TIME = int(len(data) * 0.8) # DO NOT CHANGE THIS\n",
        "    x_train = data[:SPLIT_TIME]\n",
        "    x_valid = data[SPLIT_TIME:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
        "    # In other cases, it is advised not to change the batch size since it\n",
        "    # might affect your final scores. While setting it to a lower size\n",
        "    # might not do any harm, higher sizes might affect your scores.\n",
        "    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "\n",
        "    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "    # on the server.\n",
        "    # Number of past time steps based on which future observations should be\n",
        "    # predicted\n",
        "    N_PAST = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Number of future time steps which are to be predicted.\n",
        "    N_FUTURE = 10  # DO NOT CHANGE THIS\n",
        "\n",
        "    # By how many positions the window slides to create a new window\n",
        "    # of observations.\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Code to create windowed train and validation datasets.\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "\n",
        "    # Code to define your model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "            Conv1D(filters=32, kernel_size=5, padding='causal', activation='relu', input_shape=[N_PAST, 1]),\n",
        "            Bidirectional(LSTM(32, return_sequences=True)),\n",
        "            Bidirectional(LSTM(32, return_sequences=True)),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(N_FEATURES)\n",
        "        # ADD YOUR LAYERS HERE.\n",
        "\n",
        "        # If you don't follow the instructions in the following comments,\n",
        "        # tests will fail to grade your code:\n",
        "        # The input layer of your model must have an input shape of:\n",
        "        # (BATCH_SIZE, N_PAST = 10, N_FEATURES = 1)\n",
        "        # The model must have an output shape of:\n",
        "        # (BATCH_SIZE, N_FUTURE = 10, N_FEATURES = 1).\n",
        "        # Make sure that there are N_FEATURES = 1 neurons in the final dense\n",
        "        # layer since the model predicts 1 feature.\n",
        "\n",
        "        # HINT: Bidirectional LSTMs may help boost your score. This is only a\n",
        "        # suggestion.\n",
        "\n",
        "        # WARNING: After submitting the trained model for scoring, if you are\n",
        "        # receiving a score of 0 or an error, please recheck the input and\n",
        "        # output shapes of the model to see if it exactly matches our requirements.\n",
        "        # The grading infrastructure is very strict about the shape requirements.\n",
        "        # Most common issues occur when the shapes are not matching our\n",
        "        # expectations.\n",
        "        #\n",
        "        # TIP: You can print the output of model.summary() to review the model\n",
        "        # architecture, input and output shapes of each layer.\n",
        "        # If you have made sure that you have matched the shape requirements\n",
        "        # and all the other instructions we have laid down, and still\n",
        "        # receive a bad score, you must work on improving your model.\n",
        "\n",
        "        # WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "        # recurrent_dropout argument (you can alternatively set it to 0),\n",
        "        # since it has not been implemented in the cuDNN kernel and may\n",
        "        # result in much longer training times.\n",
        "        # tf.keras.layers.Dense(N_FEATURES)\n",
        "    ])\n",
        "    checkpoint_path = 'model/my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_mae',\n",
        "                             verbose=1)\n",
        "    # Code to train and compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam(0.0001)\n",
        "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
        "    model.fit(train_set,\n",
        "          validation_data=(valid_set),\n",
        "          epochs=100,\n",
        "          callbacks=[checkpoint])\n",
        "\n",
        "    return model\n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n",
        "\n",
        "\n",
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "\n",
        "#def model_forecast(model, series, window_size, batch_size):\n",
        "#    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "#    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "#    ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "#    forecast = model.predict(ds)\n",
        "#    return forecast\n",
        "\n",
        "# PASS THE NORMALIZED data IN THE FOLLOWING CODE\n",
        "\n",
        "# rnn_forecast = model_forecast(model, data, N_PAST, BATCH_SIZE)\n",
        "# rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0, 0]\n",
        "\n",
        "# x_valid = np.squeeze(x_valid[:rnn_forecast.shape[0]])\n",
        "# result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\n"
      ],
      "metadata": {
        "id": "TWL5Wvrlk4fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# X"
      ],
      "metadata": {
        "id": "PxnQYDUtkKQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "#\n",
        "# BASIC DATASETS QUESTION\n",
        "#\n",
        "# Create a classifier for the German Traffic Signs dataset that classifies\n",
        "# images of traffic signs into 43 classes.\n",
        "# ==============================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# The dataset contains traffic sign boards from the streets captured into\n",
        "# image files. There are 43 unique classes in total. The images are of shape\n",
        "# (30,30,3).\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# We have already divided the data for training and validation.\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. preprocess()\n",
        "# 2. solution_model()\n",
        "#\n",
        "# Your code will fail to be graded if the following criteria are not met:\n",
        "# 1. The input shape of your model must be (30,30,3), because the testing\n",
        "#    infrastructure expects inputs according to this specification.\n",
        "# 2. The last layer of your model must be a Dense layer with 43 neurons\n",
        "#    activated by softmax since this dataset has 43 classes.\n",
        "#\n",
        "# HINT: Your neural network must have a validation accuracy of approximately\n",
        "# 0.95 or above on the normalized validation dataset for top marks.\n",
        "\n",
        "import urllib\n",
        "import zipfile\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# This function downloads and extracts the dataset to the directory that\n",
        "# contains this file.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "# (unless you need to change https to http)\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/germantrafficsigns.zip'\n",
        "    urllib.request.urlretrieve(url, 'germantrafficsigns.zip')\n",
        "    with zipfile.ZipFile('germantrafficsigns.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def preprocess(image, label):\n",
        "    # NORMALIZE YOUR IMAGES HERE (HINT: Rescale by 1/.255)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "\n",
        "# This function loads the data, normalizes and resizes the images, splits it into\n",
        "# train and validation sets, defines the model, compiles it and finally\n",
        "# trains the model. The trained model is returned from this function.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION.\n",
        "def solution_model():\n",
        "    # Downloads and extracts the dataset to the directory that\n",
        "    # contains this file.\n",
        "    download_and_extract_data()\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "    IMG_SIZE = 30\n",
        "\n",
        "    # The following code reads the training and validation data from their\n",
        "    # respective directories, resizes them into the specified image size\n",
        "    # and splits them into batches. You must fill in the image_size\n",
        "    # argument for both training and validation data.\n",
        "    # HINT: Image size is a tuple\n",
        "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory='train/',\n",
        "        label_mode='categorical',\n",
        "        image_size=  IMG_SIZE,\n",
        "        batch_size = BATCH_SIZE)\n",
        "\n",
        "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory='validation/',\n",
        "        label_mode='categorical',\n",
        "        image_size= IMG_SIZE,\n",
        "        batch_size = BATCH_SIZE)\n",
        "\n",
        "    # Normalizes train and validation datasets using the\n",
        "    # preprocess() function.\n",
        "    # Also makes other calls, as evident from the code, to prepare them for\n",
        "    # training.\n",
        "    # Do not batch or resize the images in the dataset here since it's already\n",
        "    # been done previously.\n",
        "\n",
        "    train_ds = train_ds.map(\n",
        "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(\n",
        "        tf.data.experimental.AUTOTUNE)\n",
        "    val_ds = val_ds.map(\n",
        "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Code to define the model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # ADD LAYERS OF THE MODEL HERE\n",
        "\n",
        "        # If you don't adhere to the instructions in the following comments,\n",
        "        # tests will fail to grade your model:\n",
        "        # The input layer of your model must have an input shape of\n",
        "        # (30,30,3).\n",
        "        # Make sure your last layer has 43 neurons activated by softmax.\n",
        "        tf.keras.layers.Dense(43, activation=tf.nn.softmax)\n",
        "    ])\n",
        "\n",
        "    # Code to compile and train the model\n",
        "    model.compile(\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "metadata": {
        "id": "utgLrtgjEmiA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}